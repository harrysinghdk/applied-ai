{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["%"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torchvision\n", "import torchvision.datasets"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn"]}, {"cell_type": "markdown", "metadata": {}, "source": ["simple toy model to have something to work with"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = torch.nn.Sequential(\n", "    torch.nn.Conv2d(1, 16, kernel_size=3, padding=0),\n", "    torch.nn.ReLU(),\n", "    torch.nn.MaxPool2d(kernel_size=3, stride=2),\n", "    torch.nn.Flatten(),\n", "    torch.nn.Linear(16 * 3 * 3, 10)\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["load the MNIST dataset<br>\n", "this comes with all torch niceties and helpers, but we will try to get the<br>\n", "raw data from here (not all the way to the file system, but close enough)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dataset = torchvision.datasets.MNIST(\n", "    root='../data/mnist',\n", "    train=True\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["here we extract the data from the dataset and we store it as a simple array of numbers<br>\n", "DO NOT DO THIS, this is just for explanation purposes"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dataset_numpy = zip(np.array(dataset.data), np.array(dataset.targets))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["here we create some simple examples of preprocessing operations"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["transform_np_image_to_tensor = torchvision.transforms.ToTensor()\n", "transform_np_vector_to_tensor = torch.tensor"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def training_loop(num_epoch, model, dataset, loss_f, optimizer):\n", "    for epoch in range(num_epoch):\n", "        for images, labels in dataset:\n", "            # since we don't have the torch Dataset taking care of\n", "            # this for us under the hood, we need to do this ourselves\n", "            # this approach works but it's not very good\n", "            preprocessed_input  = transform_np_image_to_tensor(images)\n", "            preprocessed_labels = transform_np_vector_to_tensor(labels)\n", "            evaluation_output = model(preprocessed_input)\n\n", "            # you should be able to finish this training loop based on the last\n", "            # three weeks exercises\n", "            # what is missing is\n", "            # - compute loss\n", "            # - do backpropagation\n", "            # - do evaluation\n", "            # feel free to ask if you have troubles with this\n", "   "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["training_loop(\n", "    10,\n", "    model,\n", "    dataset_numpy,\n", "    loss_f = torch.nn.L1Loss(),\n", "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["%<br>\n", "additional exercise (if this is of interest to you)<br>\n", "based of the CelebA_Local dataset provided in the solution for exercise 4,<br>\n", "can you build a data pipeline yourself using torch datasets?<br>\n", "try to use it to do some of the tasks outlined in exercise 5"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}