{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "J2qzt2TyXVSG",
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Neural Networks\n",
    "### Hand-in: Solve MNIST classification with basic tensorflow or pytorch\n",
    "\n",
    "The goal of this exercise is to learn one of the current state-of-the-art frameworks (tensorflow and pytorch) for using neural network architectures and training.\n",
    "https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/quickstart/beginner.ipynb\n",
    "https://colab.research.google.com/github/omarsar/pytorch_notebooks/blob/master/pytorch_quick_start.ipynb\n",
    "\n",
    "We will start off with MNIST from the basic exercise for you to reproduce the results here. Important then is to carefully go through the API and tutorial of your framework of choice and put together an MLP that can classify MNIST.\n",
    "\n",
    "For the hand-in, go through the following tasks and answer the following questions.\n",
    "\n",
    "Tasks:\n",
    "#### 1. Go through the API to understand what documentation and details you can find\n",
    "#### 2. Learn how to set up and use a data loader.\n",
    "#### 3. Figure out and reimplement how to build a basic MLP model.\n",
    "#### 4. Understand and test how to set up the prerequisites of a model: data(loader), task & metrics.\n",
    "#### 5. Understand and test what the respective loss function and optimisers are.\n",
    "#### 6. Plot some training progress (e.g. plot the loss)\n",
    "#### 7. Have a brief (!) experiment with different settings for the hyperparameters: batch_size, learning_rate, hidden_layer_sizes.\n",
    "\n",
    "Questions:\n",
    "1. What is the best accuracy that you found?\n",
    "2. What are good values of batch_size, learning_rate, hidden_layer_sizes - is this the same compared to the basic (sklearn) training?\n",
    "3. What optimiser options do you have, and which one did you choose (and why)?\n",
    "4. What did you observe in the results for good and bad hyperparameters in comparison to the basic (sklearn) training?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "iESVUUIIXVSH",
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "Before you start, you need to enable all commented lines of code for the framework of your choice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hKAsJkJhXVSH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByTKw4nEXVSI",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Load MNIST directly from tensorflow or pytorch\n",
    "Make sure to use training and test (and potentially validation) data properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "le1evOzXXVSI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Tensorflow:\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "#(x_trainset, y_trainset), (x_testset, y_testset) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "x_train, x_test = x_train[..., np.newaxis]/255.0, x_test[..., np.newaxis]/255.0   # Rescale the images from [0,255] to the [0.0,1.0] range.\n",
    "\n",
    "# Pytorch:\n",
    "#trainset = torchvision.datasets.MNIST(root='../data', train=True, download=True)\n",
    "#trainset = torchvision.datasets.FashionMNIST(root='../data', train=True, download=True)\n",
    "#testset = torchvision.datasets.MNIST(root='../data', train=False, download=True)\n",
    "#testset = torchvision.datasets.FashionMNIST(root='../data', train=False, download=True)\n",
    "#trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "#testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s992RXvRXVSI",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "T0ya8INpXVSI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffcsbLPRXVSI",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "smayJK9KXVSI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWqLrATlXVSJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QBARWfAfXVSJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09475642,  0.39204934, -0.2996678 ,  0.21791562, -0.5918864 ,\n",
       "         0.5064708 ,  0.49885872, -0.07770862,  0.0036536 ,  0.39944556]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model(x_train[:1]).numpy()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7sFp0coiXVSJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.07805271, 0.12700039, 0.06359107, 0.10670378, 0.04747743,\n",
       "        0.14239597, 0.14131616, 0.07939475, 0.08612455, 0.1279432 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBcasU6AXVSJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Show prediction and visualise the loss over training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 871us/step - accuracy: 0.8545 - loss: 0.4866\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 827us/step - accuracy: 0.9544 - loss: 0.1543\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 833us/step - accuracy: 0.9656 - loss: 0.1106\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 852us/step - accuracy: 0.9731 - loss: 0.0887\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 889us/step - accuracy: 0.9772 - loss: 0.0733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x32c666a50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - 535us/step - accuracy: 0.9757 - loss: 0.0766\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07660997658967972, 0.9757000207901001]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
