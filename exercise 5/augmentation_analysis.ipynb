{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "J2qzt2TyXVSG",
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Neural Networks\n",
    "### Hand-in: Study CIFAR classification with CNNs and data augmentation in tensorflow or pytorch in-depth\n",
    "\n",
    "The goal of this exercise is to explore the power of systematic data augmentation to train you in more systematically analysing your ML approach.\n",
    "\n",
    "We will look again into the more advanced task for image classification with CNNs: classifying on CIFAR10. But now, we want to include the monitoring and analysis options that are sensible to apply. If you already looked into this yesterday, then: good job! You made today's tasks much easier. Yet\n",
    "\n",
    "For the hand-in, go through the following tasks and answer the following questions.\n",
    "\n",
    "Tasks:\n",
    "1. Take your notebook from Wednesday's exercise as a starting point for this hand-in (copy+paste all your working code for data loading, model building + training, and analysis).\n",
    "2. Add some systematic data augmentation as a pre-processing step!\n",
    "3. Add k-fold cross-validation (e.g. k=5)!\n",
    "4. Add particular training monitor plots: plot loss and accuracy over the training steps and reproduce some of yesterday's as well as add more hyperparameter exploration.\n",
    "5. Try out some regularization: e.g. with adding a Normalization layer or Dropout layer [1], or (manually or trough the optimizer) add L1 or L2 regularization to your model [2].\n",
    "6. Add particular analysis steps: show a confusion matrix over all classes, plot a histogram of predictions for the classes, and plot some of the correctly vs. incorrectly classified images.\n",
    "\n",
    "Questions:\n",
    "1. Did data augmentation improve your model? Which augmentation methods were most useful (thus, you might systematically turn individual ones on/off, but DON'T put too much time into that).\n",
    "2. Did you observe that cross-validation helps for better results?\n",
    "3. Did you observe that a particular regularization helps for better results?\n",
    "4. Recapitulate: Are there classes that are difficult to classify correctly? Is this still the case with augmentation?\n",
    "5. Can you identify particular images that fail to classify consistently? Speculate: why?\n",
    "\n",
    "[1] https://pytorch.org/docs/stable/nn.html\n",
    "\n",
    "[2] Good starting point: https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-l1-l2-and-elastic-net-regularization-with-pytorch.md"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "id": "iESVUUIIXVSH",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "Note: the PRACTISE and this hand-in stub is prepared for PyTorch. You can easily find examples for TensorFlow online and are free to choose to work with TensorFlow instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 20890,
     "status": "ok",
     "timestamp": 1721325158390,
     "user": {
      "displayName": "Diana Meda",
      "userId": "03428717798582186284"
     },
     "user_tz": -120
    },
    "id": "hKAsJkJhXVSH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ByTKw4nEXVSI",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Load CIFAR-10 from tensorflow or pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1721325158393,
     "user": {
      "displayName": "Diana Meda",
      "userId": "03428717798582186284"
     },
     "user_tz": -120
    },
    "id": "le1evOzXXVSI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11773,
     "status": "ok",
     "timestamp": 1721325170160,
     "user": {
      "displayName": "Diana Meda",
      "userId": "03428717798582186284"
     },
     "user_tz": -120
    },
    "id": "_cDfUo-DKivo",
    "outputId": "cee25de2-9336-4164-d7f0-aacde1488962",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainvalset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainvalloader = torch.utils.data.DataLoader(trainvalset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "OO-Uc5T6Kivo",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Preprocessing: Data Augmentation\n",
    "Add some data augmentation steps here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2244,
     "status": "ok",
     "timestamp": 1721325242410,
     "user": {
      "displayName": "Diana Meda",
      "userId": "03428717798582186284"
     },
     "user_tz": -120
    },
    "id": "kXzNtUPRKivp",
    "outputId": "e2bf6815-4169-4b0a-dac1-38e18463fae1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# prompt: add some data augmentation steps here\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "# ## Neural Networks\n",
    "# ### Hand-in: Study CIFAR classification with CNNs and data augmentation in tensorflow or pytorch in-depth\n",
    "#\n",
    "# The goal of this exercise is to explore the power of systematic data augmentation to train you in more systematically analysing your ML approach.\n",
    "#\n",
    "# We will look again into the more advanced task for image classification with CNNs: classifying on CIFAR10. But now, we want to include the monitoring and analysis options that are sensible to apply.\n",
    "# If you already looked into this yesterday, then: good job! You made today's tasks much easier. Yet\n",
    "#\n",
    "# For the hand-in, go through the following tasks and answer the following questions.\n",
    "#\n",
    "# Tasks:\n",
    "# 1. Take your notebook from Wednesday's exercise as a starting point for this hand-in (copy+paste all your working code for data loading, model building + training, and analysis).\n",
    "# 2. Add some systematic data augmentation as a pre-processing step!\n",
    "# 3. Add k-fold cross-validation (e.g. k=5)!\n",
    "# 4. Add particular training monitor plots: plot loss and accuracy over the training steps and reproduce some of yesterday's as well as add more hyperparameter exploration.\n",
    "# 5. Try out some regularization: e.g. with adding a Normalization layer or Dropout layer [1], or (manually or trough the optimizer) add L1 or L2 regularization to your model [2].\n",
    "# 6. Add particular analysis steps: show a confusion matrix over all classes, plot a histogram of predictions for the classes, and plot some of the correctly vs. incorrectly classified images.\n",
    "#\n",
    "# Questions:\n",
    "# 1. Did data augmentation improve your model? Which augmentation methods were most useful (thus, you might systematically turn individual ones on/off, but DON'T put too much time into that).\n",
    "# 2. Did you observe that cross-validation helps for better results?\n",
    "# 3. Did you observe that a particular regularization helps for better results?\n",
    "# 4. Recapitulate: Are there classes that are difficult to classify correctly? Is this still the case with augmentation?\n",
    "# 5. Can you identify particular images that fail to classify consistently? Speculate: why?\n",
    "#\n",
    "# [1] https://pytorch.org/docs/stable/nn.html\n",
    "#\n",
    "# [2] Good starting point: https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-use-l1-l2-and-elastic-net-regularization-with-pytorch.md\n",
    "# #### Load CIFAR-10 from tensorflow or pytorch\n",
    "batch_size = 32\n",
    "transform_train = transforms.Compose(\n",
    "    [transforms.RandomCrop(32, padding=4),\n",
    "     transforms.RandomHorizontalFlip(),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainvalset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "trainvalloader = torch.utils.data.DataLoader(trainvalset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "# ##### Preprocessing: Data Augmentation\n",
    "# Add some data augmentation steps here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "s992RXvRXVSI",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Data Splitting\n",
    "Here we would do some advanced splitting like n-fold cross-validation! The code stub just gives you a head start..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "dataset = trainvalset  # mit samlede tr√¶nings+valideringsdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "Antal tr√¶ningseksempler: 40000\n",
      "Antal valideringseksempler: 10000\n",
      "\n",
      "Fold 2/5\n",
      "Antal tr√¶ningseksempler: 40000\n",
      "Antal valideringseksempler: 10000\n",
      "\n",
      "Fold 3/5\n",
      "Antal tr√¶ningseksempler: 40000\n",
      "Antal valideringseksempler: 10000\n",
      "\n",
      "Fold 4/5\n",
      "Antal tr√¶ningseksempler: 40000\n",
      "Antal valideringseksempler: 10000\n",
      "\n",
      "Fold 5/5\n",
      "Antal tr√¶ningseksempler: 40000\n",
      "Antal valideringseksempler: 10000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f\"\\nFold {fold+1}/{k}\")\n",
    "\n",
    "    # Lav subsets\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "    # Lav dataloaders\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    print(f\"Antal tr√¶ningseksempler: {len(train_subset)}\")\n",
    "    print(f\"Antal valideringseksempler: {len(val_subset)}\")\n",
    "\n",
    "    # üëâ Her tr√¶ner du din model p√• train_loader og validerer p√• val_loader\n",
    "    # fx: train_one_epoch(model, train_loader); validate(model, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "J3SPengOKivq",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Show some data characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371
    },
    "executionInfo": {
     "elapsed": 1254,
     "status": "ok",
     "timestamp": 1721325306994,
     "user": {
      "displayName": "Diana Meda",
      "userId": "03428717798582186284"
     },
     "user_tz": -120
    },
    "id": "p3HNoREqKivq",
    "outputId": "0d45d0de-dda2-4907-b981-931226d628a6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m     plt.imshow(np.transpose(npimg, (\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m)))\n\u001b[32m      7\u001b[39m     plt.show()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m dataiter = \u001b[38;5;28miter\u001b[39m(trainloader)\n\u001b[32m     10\u001b[39m images = []\n\u001b[32m     11\u001b[39m imagebatch, label = \u001b[38;5;28mnext\u001b[39m(dataiter)\n",
      "\u001b[31mNameError\u001b[39m: name 'trainloader' is not defined"
     ]
    }
   ],
   "source": [
    "NO_images = batch_size\n",
    "\n",
    "def imshow(img):\n",
    "    #img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images = []\n",
    "imagebatch, label = next(dataiter)\n",
    "\n",
    "for i in range(NO_images):\n",
    "    # draw some random images from the training set according to the dataloader\n",
    "    image = torch.squeeze(imagebatch[i], 0)\n",
    "    images.append(image)\n",
    "\n",
    "images = torch.stack(images).cpu()\n",
    "\n",
    "# print images\n",
    "print(\"This shows the next batch of images in the training set\")\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ffcsbLPRXVSI",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Build the model\n",
    "Try out regularization as layers or optimiser characteristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 346,
     "status": "ok",
     "timestamp": 1721325313522,
     "user": {
      "displayName": "Diana Meda",
      "userId": "03428717798582186284"
     },
     "user_tz": -120
    },
    "id": "smayJK9KXVSI",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleConvNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "        nn.Conv2d(1,10,kernel_size=3),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(26 * 26 * 10, 50),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(50, 20),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(20, 10)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "JWqLrATlXVSJ",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 488001,
     "status": "ok",
     "timestamp": 1721325925013,
     "user": {
      "displayName": "Diana Meda",
      "userId": "03428717798582186284"
     },
     "user_tz": -120
    },
    "id": "QBARWfAfXVSJ",
    "outputId": "bd7d5443-c177-4280-a766-d74eb3c11790",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "Antal tr√¶ningseksempler: 40000\n",
      "Antal valideringseksempler: 10000\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SimpleConvNet' object has no attribute 'SimpleConvNet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAntal tr√¶ningseksempler: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_subset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAntal valideringseksempler: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(val_subset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m model.SimpleConvNet().to(device)\n\u001b[32m     52\u001b[39m criterion = nn.CrossEntropyLoss()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ml-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1940\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1938\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1939\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1941\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1942\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'SimpleConvNet' object has no attribute 'SimpleConvNet'"
     ]
    }
   ],
   "source": [
    "# prompt: write training loop for this model\n",
    "import multiprocessing\n",
    "\n",
    "# #### Train the model\n",
    "# Try out regularization as layers or optimiser characteristic.\n",
    "class SimpleConvNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "        nn.Conv2d(3,10,kernel_size=3),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(30 * 30 * 10, 50),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(50, 20),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(20, 10)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.layers(x)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SimpleConvNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_losses = [] # bruges til at gemme vores train loss\n",
    "val_losses = []   # bruges til at gemme vores validation loss\n",
    "accuracy_list = [] # bruges til at gemme vores accuracy per epoch\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f\"\\nFold {fold+1}/{k}\")\n",
    "\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=False)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=False)\n",
    "\n",
    "    print(f\"Antal tr√¶ningseksempler: {len(train_subset)}\")\n",
    "    print(f\"Antal valideringseksempler: {len(val_subset)}\")\n",
    "\n",
    "    model.SimpleConvNet().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "dhPTHrVkKivr",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3MuQbhtSKivs",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "w4e4tIoDKivs",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Analyse some training progress\n",
    "Add plots for loss/accuracy here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otP7vOzPKivs",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "rBcasU6AXVSJ",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Analyse the model\n",
    "Add: confusion matrix, histogram over classes, plot individual images.\n",
    "Bonus: use the TSNE sample code for CIFAR10 as well. Does this help your insight?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zLDB6iIqKivs",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
