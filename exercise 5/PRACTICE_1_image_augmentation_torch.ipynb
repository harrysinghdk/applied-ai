{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRzshApmWleY"
   },
   "source": [
    "## Neural Networks\n",
    "### In-class practise: Systematic data augmentation for images\n",
    "\n",
    "by Stefan Heinrich\n",
    "Based on sec 13.1 of http://d2l.ai/chapter_computer-vision/image-augmentation.html and further adaptations by Kevin Murphy in https://github.com/probml/probml-notebooks/blob/main/notebooks-d2l/image_augmentation_torch.ipynb\n",
    "\n",
    "We illustrate some simple data augmentation methods for images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13814,
     "status": "ok",
     "timestamp": 1721324964341,
     "user": {
      "displayName": "Diana Meda",
      "userId": "03428717798582186284"
     },
     "user_tz": -120
    },
    "id": "MHkEXYIbWL6W"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(seed=1)\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "!mkdir figures # for saving plots\n",
    "\n",
    "!wget https://raw.githubusercontent.com/d2l-ai/d2l-en/master/d2l/torch.py -q -O d2l.py\n",
    "import d2l.torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "executionInfo": {
     "elapsed": 2113,
     "status": "ok",
     "timestamp": 1721324966449,
     "user": {
      "displayName": "Diana Meda",
      "userId": "03428717798582186284"
     },
     "user_tz": -120
    },
    "id": "Dmk7eNdrWnA9",
    "outputId": "d4331d96-1892-4d32-c4f2-90c533a6e4bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: https://github.com/probml/probml-notebooks/blob/main/images/cat_dog.jpg?raw=true\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'img.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m fname = \u001b[33m'\u001b[39m\u001b[33mimg.jpg\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      7\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mwget $url -q -O $fname\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m img = d2l.Image.open(fname)\n\u001b[32m     12\u001b[39m d2l.plt.imshow(img)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/ml-env/lib/python3.11/site-packages/PIL/Image.py:3513\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[32m   3512\u001b[39m     filename = os.fspath(fp)\n\u001b[32m-> \u001b[39m\u001b[32m3513\u001b[39m     fp = builtins.open(filename, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3514\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3515\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'img.jpg'"
     ]
    }
   ],
   "source": [
    "d2l.set_figsize()\n",
    "#img = d2l.Image.open('../img/cat1.jpg')\n",
    "\n",
    "#url = 'https://github.com/d2l-ai/d2l-en/blob/master/img/cat1.jpg?raw=true'\n",
    "url = 'https://github.com/probml/probml-notebooks/blob/main/images/cat_dog.jpg?raw=true'\n",
    "fname = 'img.jpg'\n",
    "!wget $url -q -O $fname\n",
    "\n",
    "\n",
    "img = d2l.Image.open(fname)\n",
    "\n",
    "d2l.plt.imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1721324966450,
     "user": {
      "displayName": "Diana Meda",
      "userId": "03428717798582186284"
     },
     "user_tz": -120
    },
    "id": "YPFwgaY_e4o1"
   },
   "outputs": [],
   "source": [
    "def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):\n",
    "    \"\"\"Plot a list of images.\"\"\"\n",
    "    figsize = (num_cols * scale, num_rows * scale)\n",
    "    fig, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    for i, (ax, img) in enumerate(zip(axes, imgs)):\n",
    "        if torch.is_tensor(img):\n",
    "            # Tensor Image\n",
    "            ax.imshow(img.numpy())\n",
    "        else:\n",
    "            # PIL Image\n",
    "            ax.imshow(img)\n",
    "        ax.axes.get_xaxis().set_visible(False)\n",
    "        ax.axes.get_yaxis().set_visible(False)\n",
    "        if titles:\n",
    "            ax.set_title(titles[i])\n",
    "    plt.tight_layout()\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Hnk_4jTXJ6t"
   },
   "source": [
    "To visualize an image augmentation, which may be stochastic, we apply it multiple times to an image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1721324977957,
     "user": {
      "displayName": "Diana Meda",
      "userId": "03428717798582186284"
     },
     "user_tz": -120
    },
    "id": "ViWltqFPXK5T"
   },
   "outputs": [],
   "source": [
    "\n",
    "def apply(img, aug, num_rows=1, num_cols=4, scale=2):\n",
    "    Y = [aug(img) for _ in range(num_rows * num_cols)]\n",
    "    fig, axes = show_images(Y, num_rows, num_cols, scale=scale)\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHSpLphPXX7w"
   },
   "source": [
    "# Flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 1505,
     "status": "ok",
     "timestamp": 1721324983056,
     "user": {
      "displayName": "Diana Meda",
      "userId": "03428717798582186284"
     },
     "user_tz": -120
    },
    "id": "dOfXdCB4XYsP",
    "outputId": "03036341-b1d8-4813-dd49-2f4640c9f124"
   },
   "outputs": [],
   "source": [
    "apply(img, torchvision.transforms.RandomHorizontalFlip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 1098,
     "status": "ok",
     "timestamp": 1721324991355,
     "user": {
      "displayName": "Diana Meda",
      "userId": "03428717798582186284"
     },
     "user_tz": -120
    },
    "id": "kcTMBFt6XZTc",
    "outputId": "6cfbecf3-850f-4a67-ce36-dc842fc9d755"
   },
   "outputs": [],
   "source": [
    "apply(img, torchvision.transforms.RandomVerticalFlip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F53SuTQpXr63"
   },
   "source": [
    "# Crop and resize\n",
    "\n",
    "Below, we randomly crop a region with an area of 10% to 100% of the original area, and the ratio of width to height of the region is randomly selected from between 0.5 and 2. Then, the width and height of the region are both scaled to 200 pixels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "executionInfo": {
     "elapsed": 1093,
     "status": "ok",
     "timestamp": 1721324994216,
     "user": {
      "displayName": "Diana Meda",
      "userId": "03428717798582186284"
     },
     "user_tz": -120
    },
    "id": "wAnkJLjMXqln",
    "outputId": "5f6371f4-9e99-4974-8d6e-ccd770663e5e"
   },
   "outputs": [],
   "source": [
    "shape_aug = torchvision.transforms.RandomResizedCrop(\n",
    "    (200, 200), scale=(0.1, 1), ratio=(0.5, 2))\n",
    "apply(img, shape_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "executionInfo": {
     "elapsed": 1164,
     "status": "ok",
     "timestamp": 1721324998934,
     "user": {
      "displayName": "Diana Meda",
      "userId": "03428717798582186284"
     },
     "user_tz": -120
    },
    "id": "SGXiBO7PdqiL",
    "outputId": "9a1849c1-9b98-4b6e-ca12-32f1b137517f"
   },
   "outputs": [],
   "source": [
    "shape_aug = torchvision.transforms.RandomResizedCrop(\n",
    "    (200, 200), scale=(0.5, 1), ratio=(1, 1))\n",
    "fig, axes = apply(img, shape_aug)\n",
    "fig.savefig('dog_cat_augment.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjO-R8l2YI6w"
   },
   "source": [
    "# Changing color\n",
    "\n",
    "We can change brightness, contrast, saturation and hue.\n",
    "First we change brightness, from 1-0.5=0.5 times less to 1+0.5=1.5 times more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 1073,
     "status": "ok",
     "timestamp": 1721325006677,
     "user": {
      "displayName": "Diana Meda",
      "userId": "03428717798582186284"
     },
     "user_tz": -120
    },
    "id": "AcrnnvKxX_Hu",
    "outputId": "93886ab4-b050-4d67-f5b6-73b44116798f"
   },
   "outputs": [],
   "source": [
    "apply(\n",
    "    img,\n",
    "    torchvision.transforms.ColorJitter(brightness=0.5, contrast=0,\n",
    "                                       saturation=0, hue=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_MtkzuxYd2W"
   },
   "source": [
    "Now we change hue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 1129,
     "status": "ok",
     "timestamp": 1721325013349,
     "user": {
      "displayName": "Diana Meda",
      "userId": "03428717798582186284"
     },
     "user_tz": -120
    },
    "id": "87QsG4iRYehk",
    "outputId": "83fd2a9a-2902-4d79-b1de-b5856f381e2b"
   },
   "outputs": [],
   "source": [
    "apply(\n",
    "    img,\n",
    "    torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0,\n",
    "                                       hue=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lw5aKuh9YgDP"
   },
   "source": [
    "Now we change saturation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 1154,
     "status": "ok",
     "timestamp": 1721325020295,
     "user": {
      "displayName": "Diana Meda",
      "userId": "03428717798582186284"
     },
     "user_tz": -120
    },
    "id": "6JGNK9UuYfHc",
    "outputId": "77a14c58-1273-4fb9-8ac8-228e183fc134"
   },
   "outputs": [],
   "source": [
    "apply(\n",
    "    img,\n",
    "    torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0.5,\n",
    "                                       hue=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwT0NxPmYm7z"
   },
   "source": [
    "Now we change contrast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 1322,
     "status": "ok",
     "timestamp": 1721325025041,
     "user": {
      "displayName": "Diana Meda",
      "userId": "03428717798582186284"
     },
     "user_tz": -120
    },
    "id": "iIHucho-Yj0q",
    "outputId": "b3116823-4d7c-4cce-b299-2039807710af"
   },
   "outputs": [],
   "source": [
    "apply(\n",
    "    img,\n",
    "    torchvision.transforms.ColorJitter(brightness=0, contrast=0.5, saturation=0,\n",
    "                                       hue=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZK46MZfXYtiT"
   },
   "source": [
    "Now we change all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 1368,
     "status": "ok",
     "timestamp": 1721325027555,
     "user": {
      "displayName": "Diana Meda",
      "userId": "03428717798582186284"
     },
     "user_tz": -120
    },
    "id": "96DSwPZhYpZU",
    "outputId": "bd77cd75-b64c-4479-f0e4-6d8e2be30335"
   },
   "outputs": [],
   "source": [
    "color_aug = torchvision.transforms.ColorJitter(brightness=0.5, contrast=0.5,\n",
    "                                               saturation=0.5, hue=0.5)\n",
    "apply(img, color_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6B-m6MFuY4Rx"
   },
   "source": [
    "# Combining multiple augmentations in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "executionInfo": {
     "elapsed": 1087,
     "status": "ok",
     "timestamp": 1721325033158,
     "user": {
      "displayName": "Diana Meda",
      "userId": "03428717798582186284"
     },
     "user_tz": -120
    },
    "id": "15Md8QlfYzof",
    "outputId": "996afb17-99d3-431f-cab2-2b684da5374e"
   },
   "outputs": [],
   "source": [
    "augs = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomHorizontalFlip(), color_aug, shape_aug])\n",
    "apply(img, augs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "executionInfo": {
     "elapsed": 1183,
     "status": "ok",
     "timestamp": 1721325048092,
     "user": {
      "displayName": "Diana Meda",
      "userId": "03428717798582186284"
     },
     "user_tz": -120
    },
    "id": "RFooEcBbgwaq",
    "outputId": "23689888-f333-4737-b48c-77896983a82c"
   },
   "outputs": [],
   "source": [
    "augs = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomHorizontalFlip(), shape_aug])\n",
    "fig, axes = apply(img, augs)\n",
    "fig.savefig('dog_cat_augment2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qE9v8ePHZGMp"
   },
   "source": [
    "# Using augmentations in a dataloader\n",
    "\n",
    "We illustrate how we can transform training and test images from FashionMNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 631
    },
    "executionInfo": {
     "elapsed": 8807,
     "status": "ok",
     "timestamp": 1721325062181,
     "user": {
      "displayName": "Diana Meda",
      "userId": "03428717798582186284"
     },
     "user_tz": -120
    },
    "id": "XCxDuARqY6Om",
    "outputId": "ebbb7c26-29c9-4e30-ea54-a87fbda26935"
   },
   "outputs": [],
   "source": [
    "all_images = torchvision.datasets.FashionMNIST(train=True, root=\"../data\",\n",
    "                                          download=True)\n",
    "d2l.show_images([all_images[i][0] for i in range(32)], 4, 8, scale=0.8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1721325063166,
     "user": {
      "displayName": "Diana Meda",
      "userId": "03428717798582186284"
     },
     "user_tz": -120
    },
    "id": "xWc_aZ5EZLfZ"
   },
   "outputs": [],
   "source": [
    "train_augs = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor()])\n",
    "\n",
    "test_augs = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1721325069467,
     "user": {
      "displayName": "Diana Meda",
      "userId": "03428717798582186284"
     },
     "user_tz": -120
    },
    "id": "9ZqpRWixZPEc",
    "outputId": "d0d8fad5-216f-4102-f759-b075ff9d43b7"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = torchvision.datasets.FashionMNIST(root=\"../data\", train=True,\n",
    "                                       transform=train_augs, download=True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True,\n",
    "    num_workers=d2l.get_dataloader_workers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2LLZXMeZRgU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "NO_images = batch_size\n",
    "\n",
    "def imshow(img):\n",
    "    #img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(dataloader)\n",
    "images, label = next(dataiter)\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "# print images\n",
    "print(\"This shows the next batch of images in the training set\")\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
