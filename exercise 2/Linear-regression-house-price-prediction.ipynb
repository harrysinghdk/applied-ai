{"cells":[{"cell_type":"markdown","metadata":{"id":"PAawTE9ogeDt"},"source":["> We are going to use the `USA_Housing` or `sample_data/california_housing...` dataset. Since house price is a continues variable, this is a regression problem. The data contains the following columns:\n","\n","> * '`Avg. Area Income`': Avg. Income of residents of the city house is located in.\n","> * '`Avg. Area House Age`': Avg Age of Houses in same city\n","> * '`Avg. Area Number of Rooms`': Avg Number of Rooms for Houses in same city\n","> * '`Avg. Area Number of Bedrooms`': Avg Number of Bedrooms for Houses in same city\n","> * '`Area Population`': Population of city hou  se is located in\n","> * '`Price`': Price that the house sold at\n","> * '`Address`': Address for the house\n","\n","Let's try and create a model that can predict the price of a house based on the variables and historical data\n"]},{"cell_type":"markdown","metadata":{"id":"7vsxOB1PgeDw"},"source":["# Import standard libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sX3VKbwUgeDw","trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","\n","sns.set_style(\"whitegrid\")\n","plt.style.use(\"fivethirtyeight\")"]},{"cell_type":"markdown","metadata":{"id":"jnsnvsT7geDx"},"source":["## Explore the data a bit.\n","### List:\n","\n","1.   First few rows\n","2.   Basic statistic\n","3.   .info()\n","4.   Column names\n","\n","Questions:\n","\n","1. What is the dependent variable (column name)?\n","2. What are the independent variables?\n","3. Should we normalize the data?\n","4. What are the column data-types?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uvRR8f-zimSx"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Zx6M9jL9geDz"},"source":["# Exploratory Data Analysis (EDA)\n","\n","Create some simple plots to check out the data!\n","\n","1.   Plot the pairwise scatter-plot between each column\n","2.   Plot the distribution of the values of the dependent variable\n","3.   Plot the pairwise correlation heatmap of each column.\n","\n","Answer questions:\n","\n","1.  What are the assumptions of the linear regression model?\n","2.  Can we accept the basic assumptions of the linear regression model?\n","3.  Judging by the scatter-plots, do you see any patterns in the data?\n","4.  Judging by the correlation heat-map, is there correlation between the dependent variable and the independent variables?\n","5.  Are there correlations among independent variables?\n"]},{"cell_type":"markdown","metadata":{"id":"oT5_Qj3TgeD0"},"source":["# Training a Linear Regression Model\n","## X and y arrays\n","\n","Tasks:\n","\n","1. Split the data-frame into an `X` array and and `y` array\n","2. Are all columns useful for linear regression? Is there some that should be excluded? Exclude the useless ones"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ZiD9CkugeD0","trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"DRCbofl2geD1"},"source":["## Train / Test Split\n","\n","We need to split the data into the train and test subsets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qlloZgy0geD1","trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Split the data here"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Or4iUIodgeD1","trusted":true},"outputs":[],"source":["from sklearn import metrics\n","\n","def print_evaluate(true, predicted):\n","    #Print mean absolute error\n","    print('MAE:', mae)\n","    #Print mean square error\n","    print('MSE:', mse)\n","    # Print root square error\n","    print('RMSE:', rmse)\n","    # Print R2 score\n","    print('R2 Square', r2_square)\n","    print('__________________________________')\n","\n","def evaluate(true, predicted):\n","    # Fill in the code necessary to calculate these metrics and use this\n","    # function to print them in the previous function\n","    return mae, mse, rmse, r2_square"]},{"cell_type":"markdown","metadata":{"id":"OqGseYoygeD2"},"source":["# Preparing Data For Linear Regression\n","> Linear regression is been studied at great length, and there is a lot of literature on how your data must be structured to make best use of the model.\n","\n","> As such, there is a lot of sophistication when talking about these requirements and expectations. In practice, you can uses these rules more as rules of thumb when using Ordinary Least Squares Regression, the most common implementation of linear regression.\n","\n","> Try different preparations of your data using these heuristics and see what works best for your problem.\n","\n","Assess the following steps using the plots you made earlier and answer whether the assumtions and conditions are being met:\n","\n","- **Linear Assumption.** Linear regression assumes that the relationship between your input and output is linear. You may need to transform data to make the relationship linear (e.g. log transform for an exponential relationship).\n","\n","Answer:\n","\n","- **Rescale Inputs:** Linear regression will often make more reliable predictions if you rescale input variables using standardization or normalization.\n","\n","Answer:\n","\n","- **Remove Collinearity.** Linear regression will over-fit your data when you have highly correlated input variables. Consider calculating pairwise correlations for your input data and removing the most correlated.\n","\n","Answer:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W_Kn2BIkgeD2","trusted":true},"outputs":[],"source":["# Hint:\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline"]},{"cell_type":"markdown","metadata":{"id":"tZoDY0rUgeD3"},"source":["# Linear Regression\n","\n","Take the most correlated dependent/independent pair and calculate the `beta` parameters for Simple linear regression.\n","\n","Use `skitlearn` to fit the linear model and see if your calculation corresponds to what the library did.\n","\n","Use `skitlearn` to fit the linear model dependent on all independent variables (normalize inputs)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"htmNWmfrgeD3","trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"eGi2cwxWgeD3"},"source":["## Model Evaluation\n","\n","\n","Compare the error from models that used only one independent variable vs. the model that used several independent variables. Did we gain anything from including extra independent dimensions?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v0BlWxUFgeD4","trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"6BF2cLDVgeD4"},"source":["## Predictions from our Model\n","\n","Tasks:\n","\n","1.  Take the test data and plot the predicted vs. true value scatter plot.\n","\n","2.  Plot the histogram of residual values\n","\n","3.  Make a scatter plot of residuals depending on the predicted value."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8KxnK0bngeD5","trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"0IZvxleEgeD5"},"source":["## Regression Evaluation Metrics\n","\n","\n","Here are three common evaluation metrics for regression problems:\n","\n","> - **Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n","$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n","\n","> - **Mean Squared Error** (MSE) is the mean of the squared errors:\n","$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n","\n","> - **Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n","$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n","\n","> ðŸ“Œ Comparing these metrics:\n","- **MAE** is the easiest to understand, because it's the average error.\n","- **MSE** is more popular than MAE, because MSE \"punishes\" larger errors, and is easier to calculate derivations.\n","- **RMSE** RMSE is more interpretable.\n","\n","> All of these are **loss functions**,\n","\n","Tasks:\n","\n","1.  What is the interpretation of the $R^2$ score?\n","\n","2.  Pack these values into a Pandas data frame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Z5AKv8ZgeD6","trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"C6D89LJBgeD-"},"source":["# Polynomial Regression\n","> Source: [scikit-learn](http://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions)\n","\n","***\n","\n","> One common pattern within machine learning is to use linear models trained on nonlinear functions of the data. This approach maintains the generally fast performance of linear methods, while allowing them to fit a much wider range of data.\n","\n","> For example, a simple linear regression can be extended by constructing polynomial features from the coefficients. In the standard linear regression case, you might have a model that looks like this for two-dimensional data:\n","\n","$$\\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2$$\n","\n","> If we want to fit a paraboloid to the data instead of a plane, we can combine the features in second-order polynomials, so that the model looks like this:\n","\n","$$\\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + w_4 x_1^2 + w_5 x_2^2$$\n","\n","> The (sometimes surprising) observation is that this is still a linear model: to see this, imagine creating a new variable\n","\n","$$z = [x_1, x_2, x_1 x_2, x_1^2, x_2^2]$$\n","\n","> With this re-labeling of the data, our problem can be written\n","\n","$$\\hat{y}(w, x) = w_0 + w_1 z_1 + w_2 z_2 + w_3 z_3 + w_4 z_4 + w_5 z_5$$\n","\n","> We see that the resulting polynomial regression is in the same class of linear models weâ€™d considered above (i.e. the model is linear in w) and can be solved by the same techniques. By considering linear fits within a higher-dimensional space built with these basis functions, the model has the flexibility to fit a much broader range of data.\n","***\n","\n","Tasks:\n","\n","1.  Fit the data to polynomial model of degree 2\n","2.  Repeat the analysis from the simple regression and show the loss functions\n","3.  Did polynomoial regression improve the fit? how did you come to that conclusion?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9P0r3yV0geD_","trusted":true},"outputs":[],"source":["# Hint:\n","from sklearn.preprocessing import PolynomialFeatures\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":0}